# -*- coding: utf-8 -*-
"""Pure_NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18dYl3dM8oDJ0RFmsOP1467CeFnnNVbQO
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time

path = "/content/drive/MyDrive/Dataset/KaggleV2-May-2016.csv"
df = pd.read_csv(path)
df = df.copy()

df.drop(['PatientId', 'AppointmentID'], axis=1, inplace=True)
df['No-show'] = df['No-show'].map({'No': 0, 'Yes': 1})
df['Gender'] = df['Gender'].map({'M': 0, 'F': 1})

df['ScheduledDay'] = pd.to_datetime(df['ScheduledDay'])
df['AppointmentDay'] = pd.to_datetime(df['AppointmentDay'])
df['WaitingTime'] = (df['AppointmentDay'] - df['ScheduledDay']).dt.days

df = df[df['WaitingTime'] >= 0].copy()
df.loc[:, 'AppointmentWeekday'] = df['AppointmentDay'].dt.dayofweek
# df.loc[:, 'Neighbourhood'] = pd.factorize(df['Neighbourhood'])[0]

# df['Neighbourhood'] = pd.factorize(df['Neighbourhood'])[0]
# print(df['Neighbourhood'].dtype)



features = [
    'Gender', 'Age',  'Scholarship',
    'Hipertension', 'Diabetes' , 'Handcap',
    'SMS_received', 'WaitingTime', 'AppointmentWeekday'
]
# print(df[features].isnull().sum())
# print(np.isinf(df[features]).sum())
print(df[features].dtypes)

X = df[features].to_numpy()
y = df['No-show'].to_numpy()
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_std[X_std == 0] = 1

X_scaled = (X - X_mean) / X_std

# --------------------  Shuffle and Split --------------------
np.random.seed(42)
indices = np.arange(X_scaled.shape[0])
np.random.shuffle(indices)

# 80-20 split
split_idx = int(0.8 * len(indices))
train_idx, val_idx = indices[:split_idx], indices[split_idx:]

X_train, y_train = X_scaled[train_idx], y[train_idx]
X_val, y_val = X_scaled[val_idx], y[val_idx]

num_pos = np.sum(y_train)
num_neg = len(y_train) - num_pos

w0 = 1
w1 = num_neg / (num_pos)


print(w0,w1)

# w0 = len(y_train) / (2 * num_neg)
# w1 = len(y_train) / (2 * num_pos)
# print(w0, w1)

print(num_pos)
print(num_neg)

def he_initialization(n_in, n_out):
    std_dev = np.sqrt(2.0 / n_in)
    return np.random.randn(n_in, n_out) * std_dev

def xavier_initialization(n_in, n_out):
    limit = np.sqrt(6.0 / (n_in + n_out))
    return np.random.uniform(-limit, limit, (n_in, n_out))

class Layer_Dense:

    def __init__(self, n_inputs, n_neurons, initialization):
        if initialization == 'he':
            self.weights = he_initialization(n_inputs, n_neurons)
        elif initialization == 'xavier':
            self.weights = xavier_initialization(n_inputs, n_neurons)
        # self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)
        self.biases = np.zeros((1, n_neurons))

    def forward(self, inputs):
        self.inputs = inputs
        self.outputs = np.dot(inputs, self.weights) +  self.biases

    def backward(self, dvalues):
        self.dweights = np.dot(self.inputs.T, dvalues)
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)
        self.dinputs = np.dot(dvalues, self.weights.T)

class Activation_ReLU:

    def forward(self, inputs):
        self.inputs = inputs

        self.output = np.maximum(0,inputs)

    def backward(self, dvalues):
        self.dinputs = dvalues.copy()

        self.dinputs[self.inputs <= 0] = 0


class Sigmoid_Activation:

    def forward(self, inputs):
        self.inputs = inputs
        self.output = 1 / (1 + np.exp(-inputs))

    def backward(self, dvalues):
        self.dinputs = dvalues * (1 - self.output ) * self.output

class wieghted_binarycrossentropy():
    def forward(self, y_pred, y_true):
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)
        sample_losses = -(w1 * y_true * np.log(y_pred_clipped) + w0 * (1 - y_true) * np.log(1 - y_pred_clipped))
        mean_loss = np.mean(sample_losses)
        self.output = mean_loss
        return mean_loss


    def backward(self, dvalues, y_true):
        samples = len(dvalues)
        clipped_values = np.clip(dvalues, 1e-7, 1 - 1e-7)
        # self.dinputs = -(y_true / clipped_values - (1 - y_true) / (1 - clipped_values)) / samples
        self.dinputs = -(w1 * y_true / clipped_values - w0 * (1 - y_true) / (1 - clipped_values)) / samples

def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25, eps=1e-9):
    y_pred_clipped = np.clip(y_pred, eps, 1. - eps)
    pt = np.where(y_true == 1, y_pred, 1 - y_pred)
    alpha_t = np.where(y_true == 1, alpha, 1 - alpha)

    loss = -alpha_t * (1 - pt) ** gamma * np.log(pt)
    return np.mean(loss)

class Optimizer_SGD:

    def __init__(self, learning_rate = 0.03):
        self.learning_rate = learning_rate

    def update_params(self, layer):
        layer.weights += -self.learning_rate * layer.dweights
        layer.biases += -self.learning_rate * layer.dbiases

def get_batches(X, y, batch_size):
    indices = np.arange(len(X))
    np.random.shuffle(indices)
    X_shuffled, y_shuffled = X[indices], y[indices]
    for i in range(0, len(X), batch_size):

        yield X_shuffled[i:i + batch_size], y_shuffled[i:i + batch_size]

print(X_train.shape)
print(y_train.shape)

dense1 = Layer_Dense(X_train.shape[1], 64, 'he')
activation1 = Activation_ReLU()

# dense2 = Layer_Dense(128,64, 'he')
# activation2 = Activation_ReLU()


# dense2.O = Layer_Dense(64,32, 'he')
# activation2.O = Activation_ReLU()

dense3 = Layer_Dense(64, 32, 'he')
activation3 = Activation_ReLU()

dense4 = Layer_Dense(32, 1, 'xavier')
activation4 = Sigmoid_Activation()

loss_function = wieghted_binarycrossentropy()
optimizer = Optimizer_SGD()



from tqdm import trange



loss_history = []
batch_size = 1024
epochs = 1000

initial_lr = 0.1
decay_rate = 0.01
lr_history = []

start_time = time.time()
for epoch in trange(epochs,desc="Training Epochs"):
    epoch_loss = 0
    epoch_accuracy = 0
    num_batches = 0

    current_lr = initial_lr / (1 + decay_rate * epoch)
    optimizer.learning_rate = current_lr
    lr_history.append(current_lr)


    for X_batch, y_batch in get_batches(X_train, y_train, batch_size):
        # Forward pass
        dense1.forward(X_batch)
        activation1.forward(dense1.outputs)

        # dense2.forward(activation1.output)
        # activation2.forward(dense2.outputs)

        # dense2.O.forward(activation2.output)
        # activation2.O.forward(dense2.O.outputs)


        dense3.forward(activation1.output)
        activation3.forward(dense3.outputs)

        dense4.forward(activation3.output)
        activation4.forward(dense4.outputs)

        # Compute loss
        loss = loss_function.forward(activation4.output, y_batch.reshape(-1, 1))
        # epoch_loss += loss


        # Accuracy
        predictions = (activation4.output > 0.5).astype(int).flatten()
        accuracy = np.mean(predictions == y_batch)
        epoch_accuracy += accuracy

        # Backward pass
        loss_function.backward(activation4.output, y_batch.reshape(-1, 1))
        activation4.backward(loss_function.dinputs)
        dense4.backward(activation4.dinputs)
        activation3.backward(dense4.dinputs)
        dense3.backward(activation3.dinputs)
        # activation2.O.backward(dense3.dinputs)
        # dense2.O.backward(activation2.O.dinputs)
        # activation2.backward(dense3.dinputs)
        # dense2.backward(activation2.dinputs)
        activation1.backward(dense3.dinputs)
        dense1.backward(activation1.dinputs)

        # Update weights
        optimizer.update_params(dense1)
        # optimizer.update_params(dense2)
        # optimizer.update_params(dense2.O)
        optimizer.update_params(dense3)
        optimizer.update_params(dense4)
        num_batches += 1


        loss = loss_function.forward(activation4.output, y_batch.reshape(-1, 1))
        epoch_loss += loss

    loss_history.append(epoch_loss / num_batches)
    # print(f'Epoch {epoch} : {epoch_loss/num_batches}')
    
    if epoch % 100 == 0:
      # print(epoch_loss)
      print(f"Epoch {epoch}, Learning Rate: {current_lr:.6f}")
      print(f"Epoch {epoch}, Accuracy: {epoch_accuracy / num_batches:.3f}, Loss: {epoch_loss / num_batches:.3f}")
end_time = time.time()

Convergence_time = end_time - start_time
print(f"Convergence Time: {Convergence_time:.2f} seconds")

print("First logged loss (epoch 0):", loss_history[0])
print(loss_history[:5])
print(len(loss_history))

epochs = range(len(loss_history))
plt.figure(figsize=(10, 6))
plt.plot(epochs, loss_history, label='Training Loss', color='blue')
plt.title('Convergence Plot: Loss vs Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# Forward pass on validation data
dense1.forward(X_val)
activation1.forward(dense1.outputs)

# dense2.forward(activation1.output)
# activation2.forward(dense2.outputs)

# dense2.O.forward(activation2.output)
# activation2.O.forward(dense2.O.outputs)

dense3.forward(activation1.output)
activation3.forward(dense3.outputs)

dense4.forward(activation3.output)
activation4.forward(dense4.outputs)


y_pred_probs = activation4.output  
# y_pred = (y_pred_probs > 0.19465).astype(int).flatten()

# Ground truth and predicted labels
y_true = y_val.flatten()             
# y_pred = (y_pred_probs > 0.3948098606651744).astype(int).flatten() 
y_pred = (y_pred_probs >  0.5).astype(int).flatten()

print(y_pred_probs.shape)
print(y_pred.shape)
print(y_true.shape)

from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix, precision_recall_curve

import matplotlib.pyplot as plt

prec, rec, thresholds = precision_recall_curve(y_val, y_pred_probs)
plt.figure(figsize=(8, 6))
plt.plot(rec, prec)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score


fpr, tpr, thresholds = roc_curve(y_val, y_pred_probs)

roc_auc = roc_auc_score(y_val, y_pred_probs)


plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()




acc = accuracy_score(y_val, y_pred)


f1 = f1_score(y_val, y_pred)


pr_auc = average_precision_score(y_val, y_pred_probs)


recall = recall_score(y_val, y_pred)
precision = precision_score(y_val,y_pred)


cm = confusion_matrix(y_val, y_pred)


print(f"Accuracy: {acc:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print(f"PR-AUC: {pr_auc:.4f}")
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import precision_recall_curve


prec, rec, thresholds = precision_recall_curve(y_val, y_pred_probs)

with np.errstate(divide='ignore', invalid='ignore'):
    f1_scores = 2 * prec * rec / (prec + rec)
    f1_scores[np.isnan(f1_scores)] = 0

best_threshold_index = np.argmax(f1_scores)
best_threshold_value = thresholds[best_threshold_index]

print("Best F1-score:", f1_scores[best_threshold_index])
print("Best Threshold Value:", best_threshold_value)

print("Well done my boy!!!!!")

from sklearn.metrics import confusion_matrix
import seaborn as sns


cm = confusion_matrix(y_val, y_pred)


labels = ['Show', 'No-show']  


plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()

