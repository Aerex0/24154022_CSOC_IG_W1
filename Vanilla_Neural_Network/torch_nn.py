# -*- coding: utf-8 -*-
"""torch_NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PELgqS9QY4fHtBDiiLYOTy6KoTklQtgY
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
import time
from torch.optim.lr_scheduler import StepLR

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

path = '/content/drive/MyDrive/Dataset/KaggleV2-May-2016.csv'
df = pd.read_csv(path)
df = df.copy()

df.drop(['PatientId', 'AppointmentID'], axis=1, inplace=True)
df['No-show'] = df['No-show'].map({'No': 0, 'Yes': 1})
df['Gender'] = df['Gender'].map({'M': 0, 'F': 1})

df['ScheduledDay'] = pd.to_datetime(df['ScheduledDay'])
df['AppointmentDay'] = pd.to_datetime(df['AppointmentDay'])
df['WaitingTime'] = (df['AppointmentDay'] - df['ScheduledDay']).dt.days

df = df[df['WaitingTime'] >= 0].copy()
df.loc[:, 'AppointmentWeekday'] = df['AppointmentDay'].dt.dayofweek
# df.loc[:, 'Neighbourhood'] = pd.factorize(df['Neighbourhood'])[0]

# df['Neighbourhood'] = pd.factorize(df['Neighbourhood'])[0]
# print(df['Neighbourhood'].dtype)



features = [
    'Gender', 'Age',  'Scholarship',
    'Hipertension', 'Diabetes' , 'Handcap',
    'SMS_received', 'WaitingTime', 'AppointmentWeekday'
]
# print(df[features].isnull().sum())
# print(np.isinf(df[features]).sum())
# print(df[features].dtypes)

X = df[features].to_numpy()
y = df['No-show'].to_numpy()
X_mean = X.mean(axis=0)
X_std = X.std(axis=0)
X_std[X_std == 0] = 1

X_scaled = (X - X_mean) / X_std

# --------------------  Shuffle and Split --------------------
np.random.seed(42)
indices = np.arange(X_scaled.shape[0])
np.random.shuffle(indices)

# 80-20 split
split_idx = int(0.8 * len(indices))
train_idx, val_idx = indices[:split_idx], indices[split_idx:]

X_train, y_train = X_scaled[train_idx], y[train_idx]
X_val, y_val = X_scaled[val_idx], y[val_idx]

X_train = torch.FloatTensor(X_train)
X_val = torch.FloatTensor(X_val)
y_train = torch.LongTensor(y_train)
y_val = torch.LongTensor(y_val)

train_ds = TensorDataset(X_train, y_train.float())  
val_ds = TensorDataset(X_val, y_val.float())

train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True, num_workers=2)
val_loader = DataLoader(val_ds, batch_size=1024)

input_size = X_train.shape[1]
l1  = 64
l2 = 32
l3 = 1
initial_lr = 0.1
decay_rate = 0.01
num_epochs = 1000

class NeuralNet(nn.Module):
    def __init__(self, input_size,l1, l2, l3):
        super(NeuralNet, self).__init__()
        self.l1 = nn.Linear(input_size, l1)
        self.relu = nn.ReLU()
        self.l2 = nn.Linear(l1, l2)
        self.relu = nn.ReLU()
        self.l3 = nn.Linear(l2, l3)
    def forward(self, x):
        out = F.relu(self.l1(x))
        out = F.relu(self.l2(out))
        out = (self.l3(out))
        return out

pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()], dtype=torch.float32)
print(pos_weight)

model = NeuralNet(input_size, l1, l2, l3)

loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)

from tqdm import trange

loss_history = []

lr_history = []
start_time = time.time()
for epoch in trange(num_epochs, desc="Training Epochs"):
    epoch_loss = 0
    epoch_accuracy = 0
    num_batches = 0

    # Timedecay
    current_lr = initial_lr / (1 + decay_rate * epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr

    lr_history.append(current_lr)

    # Training loop
    model.train()
    for xb, yb in train_loader:
        logits = model(xb).squeeze()
        loss_value = loss(logits, yb)

        optimizer.zero_grad()
        loss_value.backward()
        optimizer.step()

        epoch_loss += loss_value.item()
        num_batches += 1

    epoch_loss /= num_batches
    loss_history.append(epoch_loss)
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}, LR: {current_lr:.6f}")

end_time = time.time()
Convergence_time = end_time - start_time
print(f"Training time: {Convergence_time:.2f} seconds")

# for epoch in trange(num_epochs):
#     model.train()
#     total_loss = 0
#     num_batches = 0
#     for xb, yb in train_loader:
#         logits = model(xb).squeeze()
#         loss_value = loss(logits, yb)

#         optimizer.zero_grad()
#         loss_value.backward()
#         optimizer.step()
#         total_loss += loss_value.item()
#         num_batches += 1
#     # loss_history.append(loss_value.item())
#     loss_history.append(total_loss/num_batches)
#     total_loss /= num_batches

#     if epoch % 100 == 0:
#         print(f"Epoch {epoch}, Loss: {total_loss / num_batches:.3f}")

print(len(loss_history))

epochs = range(len(loss_history))
plt.figure(figsize=(10, 6))
plt.plot(epochs, loss_history, label='Training Loss', color='blue')
plt.title('Convergence Plot: Loss vs Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

all_preds = []
all_probs = []
with torch.no_grad():
    for xb, yb in val_loader:
        logits = model(xb).squeeze()
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).int()
        all_preds.append(preds)
        all_probs.append(probs)

preds = torch.cat(all_preds).cpu().numpy()
pred_probs = torch.cat(all_probs).cpu().numpy()

from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix, precision_recall_curve
prec, rec, thresholds = precision_recall_curve(y_val, preds)
plt.figure(figsize=(8, 6))
plt.plot(rec, prec)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.grid(True)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score


# Calculate FPR, TPR, and thresholds
fpr, tpr, thresholds = roc_curve(y_val, pred_probs)

# Compute ROC AUC score
roc_auc = roc_auc_score(y_val, pred_probs)

# Plotting the ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})', color='blue')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix, recall_score, precision_score


acc = accuracy_score(y_val, preds)

f1 = f1_score(y_val, preds)


pr_auc = average_precision_score(y_val, pred_probs)


recall = recall_score(y_val, preds)
precision = precision_score(y_val,preds)


cm = confusion_matrix(y_val, preds)


print(f"Accuracy: {acc:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print(f"PR-AUC: {pr_auc:.4f}")
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt




cm = confusion_matrix(y_val, preds)


labels = ['Show', 'No-show']  


plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()
